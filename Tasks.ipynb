{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b115a8-9791-4698-95d4-3fe172d42708",
   "metadata": {},
   "source": [
    "# Task 1: Sentence Transformer Implementation  \n",
    "### Purpose:  \n",
    "Develop a model that encodes sentences into fixed-length embeddings.  \n",
    "This serves as the foundational step before expanding into multi-task learning.\n",
    "\n",
    "---\n",
    "\n",
    "## Thought Process  \n",
    "\n",
    "Task 1 seemed quite straightforward, just encoding sentences into embeddings. **But as I started working on it,** I realized that the choice of framework and model depends a lot on the use case and context. Sometimes, a simpler and faster model is better than a highly accurate but heavy one, especially when speed and efficiency matter more.  \n",
    "\n",
    "**For this assignment, I thought making decisions while keeping Fetch Rewards in mind would be super interesting and relevant.** Since I already understand how it works from a user’s perspective, I wanted something that was **lightweight, fast, scalable, and well-suited for short receipt entries.**  \n",
    "\n",
    "I went with `sentence-transformers` because it is built exactly for this kind of task. It handles **tokenization, pooling, and encoding** in one go, so I did not have to set all that up manually.  \n",
    "\n",
    "- I considered using **raw PyTorch**, but that would mean coding the entire pipeline myself, which felt unnecessary for something this simple.  \n",
    "- **Hugging Face’s transformers library** was another option, but using it directly would require manually defining tokenizers and pooling.  \n",
    "- Since `sentence-transformers` is **PyTorch-based and leverages Hugging Face models under the hood**, it made the most sense.  \n",
    "\n",
    "Beyond choosing **MiniLM** as the transformer backbone, I also had to decide **how to process the embeddings efficiently.**  \n",
    "\n",
    "- I used **MiniLM’s built-in mean pooling** rather than max pooling or CLS token representation, as it provides a **balanced and stable sentence-level representation.**  \n",
    "- **Since no specific training data was provided,** I opted to use MiniLM’s **pretrained embeddings directly,** prioritizing **speed and generalization** over task-specific adaptation.  \n",
    "- While some models struggle with **noisy text or typos**, from my experience, receipt data is usually well-structured and does not commonly have such issues (I'm not covering complex use cases like promotional discounts in this task) Given this, **MiniLM’s pretrained embeddings should generalize well without requiring additional fine-tuning.**  \n",
    "\n",
    "---\n",
    "\n",
    "## Model Selection  \n",
    "\n",
    "I picked **`all-MiniLM-L6-v2`** because:  \n",
    "\n",
    "- It is **small** with **22 million parameters** and **384-dimensional embeddings** but still performs well.  \n",
    "- I considered **BERT (110M, 768D)** and **DistilBERT (66M, 768D)**, but they seemed **overkill** for what I needed.  \n",
    "- Fetch receipts typically have **short and structured text** like `\"2x Apples $1.99\"`, so I did not need a massive model for context-heavy sentences.  \n",
    "- MiniLM is **optimized for sentence embeddings**, runs **faster**, and already has **mean pooling built in**, so I did not have to add anything extra.  \n",
    "\n",
    "---\n",
    "\n",
    "## Final Decision  \n",
    "\n",
    "In the end, I **prioritized speed and efficiency over unnecessary complexity.**  \n",
    "MiniLM gave me **solid embeddings without slowing things down**, which made it the best choice for this task.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3da51f-207b-4daa-8e35-f3cb73df1605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter sentences to generate embeddings (type 'done' when finished):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  Tomatoes 2kgs\n",
      ">  Olive oil 500ml\n",
      ">  Brown rice 1kg\n",
      ">  Cheddar cheese 200g\n",
      ">  done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentence Embeddings:\n",
      "-----------------------------\n",
      "Sentence 1: Tomatoes 2kgs\n",
      "Embedding (first 5 of 384 dimensions): [-0.07684907  0.02557426 -0.00254072  0.09589951 -0.04464617]...\n",
      "\n",
      "Sentence 2: Olive oil 500ml\n",
      "Embedding (first 5 of 384 dimensions): [-0.04770451  0.00663916  0.01213263  0.00963966  0.08413599]...\n",
      "\n",
      "Sentence 3: Brown rice 1kg\n",
      "Embedding (first 5 of 384 dimensions): [-0.04152735  0.02855367  0.01217135  0.07473338 -0.02057862]...\n",
      "\n",
      "Sentence 4: Cheddar cheese 200g\n",
      "Embedding (first 5 of 384 dimensions): [-0.06899679  0.01976784 -0.01123568  0.04121638 -0.07709885]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Sentence Transformer Implementation\n",
    "# Purpose: Develop a model that encodes sentences into fixed-length embeddings.\n",
    "# This serves as the foundational step before expanding into multi-task learning.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Collecting receipt entries from the user\n",
    "sentences_task1 = []\n",
    "print(\"Enter sentences to generate embeddings (type 'done' when finished):\")\n",
    "\n",
    "while True:\n",
    "    sentence = input(\"> \")\n",
    "    if sentence.lower() == 'done':\n",
    "        break\n",
    "    sentences_task1.append(sentence)\n",
    "\n",
    "if not sentences_task1:\n",
    "    print(\"No sentences provided.\")\n",
    "else:\n",
    "    # Generating and displaying embeddings\n",
    "    model_task1 = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model_task1.encode(sentences_task1, batch_size=32, show_progress_bar=False)\n",
    "\n",
    "    print(\"\\nGenerated Sentence Embeddings:\")\n",
    "    print(\"-----------------------------\")\n",
    "    for i, (sentence, embedding) in enumerate(zip(sentences_task1, embeddings),1):\n",
    "        print(f\"Sentence {i}: {sentence}\")\n",
    "        print(f\"Embedding (first 5 of 384 dimensions): {embedding[:5]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e107a0-0f1e-4580-bfb0-a6700f2011f5",
   "metadata": {},
   "source": [
    "# Task 2: Multi-Task Learning Expansion\n",
    "\n",
    "While developing a multi-task learning (MTL) model, I decided to continue **keeping Fetch Rewards in mind**, ensuring that the approach **aligned with real-world receipt processing needs**. The goal was to make a single model capable of both classifying receipt items into categories and extracting numerical quantities, instead of handling these as separate tasks. Since both tasks require understanding the same structured receipt text, I thought sharing MiniLM’s embeddings across them would make the model more efficient and scalable.\n",
    "\n",
    "---\n",
    "\n",
    "# Multi-Task Learning Architecture\n",
    "\n",
    "To achieve this, I kept MiniLM as the shared transformer backbone and added two task-specific heads:\n",
    "\n",
    "- **Classification Head:** Assigns receipt items to one of five predefined categories (Fruit, Dairy, Bakery, Meat, and Other). This aligns with Fetch’s categorization system and ensures structured analysis of receipts.\n",
    "\n",
    "- **Quantity Extraction Head:** Identifies numerical values in text, such as recognizing \"2\" in \"2x Apples $1.99.\" I thought this might be useful for purchase tracking, optimizing spending insights, and enhancing Fetch’s reward calculations.\n",
    "\n",
    "---\n",
    "\n",
    "# Implementation Details\n",
    "\n",
    "- A **fully connected classification layer (nn.Linear(384, 5))** to map MiniLM’s 384-dimensional embeddings to category logits, which are later converted into probabilities using **softmax**.\n",
    "- A **regression layer (nn.Linear(384, 1))** to predict a single scalar value representing the quantity.\n",
    "- A **shared MiniLM backbone** so that the model processes each receipt once while generating outputs for both tasks.\n",
    "- **Independent task heads** to **minimize task interference**, ensuring that learning signals from one task do not negatively impact the other.\n",
    "\n",
    "---\n",
    "  \n",
    "# Training Considerations\n",
    "\n",
    "Since no labeled training data was provided, I used MiniLM’s pretrained embeddings directly instead of fine-tuning. Fine-tuning would have been valuable if I had access to a Fetch-specific dataset, but in this case, leveraging pretrained embeddings seemed like the most practical choice. In the end, this approach allows the model to classify receipt items and extract numerical quantities in a single forward pass, making it efficient, scalable, and well-suited for Fetch’s use case atleast for time being (Please note, training the heads is still necessary moving forward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adbfd694-452e-42f4-9486-2538ba47487d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTL model initialized.\n",
      "Processing the following receipt items:\n",
      "1. 2x Apples $1.99\n",
      "2. Milk 1L $2.50\n",
      "3. 3x Bread $3.00\n",
      "4. Chicken Breast 500g $5.99\n",
      "5. 4x Bananas $2.40\n",
      "\n",
      "Multi-Task Predictions:\n",
      "----------------------\n",
      "Sentence 1: 2x Apples $1.99\n",
      "Predicted Category: Meat\n",
      "Predicted Quantity: -0.01\n",
      "\n",
      "Sentence 2: Milk 1L $2.50\n",
      "Predicted Category: Fruit\n",
      "Predicted Quantity: -0.04\n",
      "\n",
      "Sentence 3: 3x Bread $3.00\n",
      "Predicted Category: Dairy\n",
      "Predicted Quantity: -0.02\n",
      "\n",
      "Sentence 4: Chicken Breast 500g $5.99\n",
      "Predicted Category: Dairy\n",
      "Predicted Quantity: -0.00\n",
      "\n",
      "Sentence 5: 4x Bananas $2.40\n",
      "Predicted Category: Meat\n",
      "Predicted Quantity: 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Multi-Task Learning Expansion\n",
    "# Purpose: Extend MiniLM for multi-task learning with two heads:\n",
    "# 1. Product category classification (Task A)\n",
    "# 2. Quantity extraction (Task B)\n",
    "\n",
    "# Importing required libraries again (to avoid running them out of order)\n",
    "import torch  # For neural network operations\n",
    "import torch.nn as nn  # Neural network module\n",
    "from sentence_transformers import SentenceTransformer  # For MiniLM backbone\n",
    "\n",
    "# Defining the multi-task model class\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_categories=5):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        #Loading the pre-trained MiniLM model\n",
    "        self.backbone = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        #Embedding size from MiniLM (fixed at 384 dimensions by defaut)\n",
    "        self.embedding_dim = 384\n",
    "        #Task A: Classification head (I chose 5 categories)\n",
    "        self.classification_head = nn.Linear(self.embedding_dim, num_categories)\n",
    "        #Task B: Regression head (quantity prediction)\n",
    "        self.regression_head = nn.Linear(self.embedding_dim, 1)\n",
    "    \n",
    "    def forward(self, sentences):\n",
    "        #Encoding sentences into embeddings\n",
    "        embeddings = self.backbone.encode(sentences, convert_to_tensor=True, \n",
    "                                        batch_size=32, show_progress_bar=False)\n",
    "        # Ensuring embeddings match device (e.g., CPU/GPU)\n",
    "        embeddings = embeddings.to(self.classification_head.weight.device)\n",
    "        # Task A: Output classification logits\n",
    "        class_logits = self.classification_head(embeddings)\n",
    "        # Task B: Output quantity prediction\n",
    "        quantity_pred = self.regression_head(embeddings)\n",
    "        return class_logits, quantity_pred\n",
    "\n",
    "# Initializing the model with 5 categories\n",
    "model_task2 = MultiTaskModel(num_categories=5)\n",
    "category_labels = [\"Fruit\", \"Dairy\", \"Bakery\", \"Meat\", \"Other\"]\n",
    "\n",
    "# Confirming the setup\n",
    "print(\"MTL model initialized.\")\n",
    "\n",
    "# Using a hardcoded list of 5 receipt-like sentences for testing\n",
    "sentences_task2 = [\n",
    "    \"2x Apples $1.99\",    \n",
    "    \"Milk 1L $2.50\",     \n",
    "    \"3x Bread $3.00\",    \n",
    "    \"Chicken Breast 500g $5.99\",  \n",
    "    \"4x Bananas $2.40\"    \n",
    "]\n",
    "\n",
    "# Displaying the sentences to be processed for clarity\n",
    "print(\"Processing the following receipt items:\")\n",
    "for i, sentence in enumerate(sentences_task2, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "\n",
    "# Get predictions (since no training has been performed on heads, they are expected to give random results)\n",
    "model_task2.eval()\n",
    "with torch.no_grad():\n",
    "    class_logits, quantity_pred = model_task2(sentences_task2)\n",
    "    class_probs = torch.softmax(class_logits, dim=1)\n",
    "    class_indices = torch.argmax(class_probs, dim=1)\n",
    "    \n",
    "    # Displaying results\n",
    "    print(\"\\nMulti-Task Predictions:\")\n",
    "    print(\"----------------------\")\n",
    "    for i, (sentence, category_idx, qty) in enumerate(zip(sentences_task2, class_indices, quantity_pred)):\n",
    "        category = category_labels[category_idx.item()]\n",
    "        quantity = qty.item()\n",
    "        print(f\"Sentence {i+1}: {sentence}\")\n",
    "        print(f\"Predicted Category: {category}\")\n",
    "        print(f\"Predicted Quantity: {quantity:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a330c-6971-4dd2-82bb-ae4f473d9fb7",
   "metadata": {},
   "source": [
    "# Task 3: Adapting MiniLM for Multi-Task Learning\n",
    "\n",
    "*Please refer to **Task_Approach** for detailed explanation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b0898-8df7-4c68-94d5-e785e996c18c",
   "metadata": {},
   "source": [
    "# Task 4: Training Loop Implementation (Bonus Task)\n",
    "\n",
    "For this task, I designed a **hypothetical training loop** for my **Task 2 multi-task learning (MTL) model**, which extends **MiniLM** with separate heads for **category classification** and **quantity extraction**. Since this is a conceptual exercise, I focused on **structuring an efficient MTL training process**, ensuring the model effectively handles **data, forward passes, and evaluation metrics**.\n",
    "\n",
    "---\n",
    "\n",
    "## Handling Hypothetical Data\n",
    "\n",
    "Given the absence of a real dataset, I assumed a **small structured dataset** of five **receipt-like sentences**, each labeled with:\n",
    "\n",
    "- **A category index** (e.g., `\"Fruit\"` mapped to class `0`).\n",
    "- **A quantity value** (e.g., `\"2x Apples\"` → Quantity: `2`).\n",
    "\n",
    "This setup **mimics real receipt data**, allowing simultaneous training of **classification and regression tasks**. **MiniLM’s tokenizer** processes inputs, and batch handling is omitted for simplicity, though in practice, a DataLoader would be used.\n",
    "\n",
    "---\n",
    "\n",
    "## Forward Pass & Model Structure\n",
    "\n",
    "To ensure a **trainable architecture**, I made the following design choices:\n",
    "\n",
    "- Used **AutoModel and a tokenizer** from **transformers** instead of `.encode()` (which isn’t differentiable).\n",
    "- **MiniLM extracts 384D embeddings** from tokenized inputs.\n",
    "- The embeddings are passed to **task-specific heads**:\n",
    "  - **Classification Head** → Outputs logits for category prediction.\n",
    "  - **Regression Head** → Predicts numerical quantities.\n",
    "\n",
    "This setup ensures **MiniLM acts as a shared encoder**, while **task-specific heads** optimize for distinct learning objectives, improving efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Functions & Metrics\n",
    "\n",
    "Each task requires a different **evaluation metric**:\n",
    "\n",
    "- **Classification:** **Accuracy** – Measures how often the predicted category matches the actual label.\n",
    "- **Regression:** **Mean Squared Error (MSE)** – Evaluates how far the predicted quantity deviates from the actual value.\n",
    "\n",
    "For **loss calculation**:\n",
    "\n",
    "- **Cross-Entropy Loss** for classification.\n",
    "- **MSE Loss** for regression.\n",
    "- **Total Loss** is computed as the sum of both losses with equal weighting **(ensuring both tasks contribute equally to learning, preventing one from dominating the training process).**\n",
    "\n",
    "### Why Cross-Entropy for Classification?\n",
    "\n",
    "I chose **Cross-Entropy Loss** because it is the standard loss function for **multi-class classification tasks**. Since the classification head outputs **logits** (raw scores before softmax), Cross-Entropy converts them into a probability distribution and penalizes incorrect predictions. It **amplifies large errors**, ensuring that misclassified samples contribute more to the loss, which helps improve learning.\n",
    "\n",
    "### Why MSE for Regression?\n",
    "\n",
    "For quantity extraction, I used **Mean Squared Error (MSE)** because it is a widely used loss function for **continuous numerical predictions**. MSE penalizes larger errors more heavily than smaller ones, making it effective for reducing variance in numerical outputs. Since the quantity values are numeric, MSE helps the model minimize **the difference between predicted and actual quantities** effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "809562a8-66c7-4bea-a0aa-9b247e2bd3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:\n",
      "Classification Loss: 1.6624, Quantity Loss: 7.0470\n",
      "Accuracy: 0.0000, MSE: 7.0470\n",
      "Epoch 2/5:\n",
      "Classification Loss: 1.4747, Quantity Loss: 1.6029\n",
      "Accuracy: 0.6000, MSE: 1.6029\n",
      "Epoch 3/5:\n",
      "Classification Loss: 1.3608, Quantity Loss: 6.2821\n",
      "Accuracy: 0.4000, MSE: 6.2821\n",
      "Epoch 4/5:\n",
      "Classification Loss: 1.5832, Quantity Loss: 6.0803\n",
      "Accuracy: 0.4000, MSE: 6.0803\n",
      "Epoch 5/5:\n",
      "Classification Loss: 1.4890, Quantity Loss: 2.2432\n",
      "Accuracy: 0.4000, MSE: 2.2432\n",
      "Training complete (hypothetical run).\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Training Loop Implementation (Bonus Task)\n",
    "# Purpose: Hypothetical training loop for Task 2 MTL model\n",
    "# Extends MiniLM with classification and quantity extraction heads\n",
    "\n",
    "# Importing libraries again to avoid running them out of order\n",
    "import torch  # For neural network operations and tensors\n",
    "import torch.nn as nn  # Neural network module\n",
    "import torch.optim as optim  # Optimizer for training\n",
    "from transformers import AutoModel, AutoTokenizer  # For differentiable MiniLM\n",
    "\n",
    "# Defining the multi-task model's class\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_categories=5):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        #Loading pre-trained MiniLM backbone (differentiable version)\n",
    "        self.backbone = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        #Loading tokenizer for MiniLM\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        #Embedding size (MiniLM’s hidden size that's default)\n",
    "        self.embedding_dim = 384\n",
    "        #Classification head (5 categories)\n",
    "        self.classification_head = nn.Linear(self.embedding_dim, num_categories)\n",
    "        #Regression head (quantity prediction)\n",
    "        self.regression_head = nn.Linear(self.embedding_dim, 1)\n",
    "    \n",
    "    def forward(self, sentences):\n",
    "        # Tokenizing sentences into input tensors (input_ids, attention_mask)\n",
    "        inputs = self.tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "        # Forward pass through MiniLM backbone\n",
    "        outputs = self.backbone(**inputs)\n",
    "        # Mean pool over sequence length to get 384D embeddings per sentence\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        #Output classification logits\n",
    "        class_logits = self.classification_head(embeddings)\n",
    "        #Output quantity prediction\n",
    "        quantity_pred = self.regression_head(embeddings)\n",
    "        return class_logits, quantity_pred\n",
    "\n",
    "# Initialize the model with 5 categories\n",
    "model = MultiTaskModel(num_categories=5)\n",
    "# Define category labels for reference (not used in training though, just for context)\n",
    "category_labels = [\"Fruit\", \"Dairy\", \"Bakery\", \"Meat\", \"Other\"]\n",
    "\n",
    "# Hypothetical dataset: 5 receipt-like sentences with labels\n",
    "# Format: (sentence, category_index, quantity)\n",
    "hypothetical_data = [\n",
    "    (\"2x Apples $1.99\", 0, 2),      # Fruit (0), qty 2\n",
    "    (\"Milk 1L $2.50\", 1, 1),        # Dairy (1), qty 1\n",
    "    (\"3x Bread $3.00\", 2, 3),       # Bakery (2), qty 3\n",
    "    (\"Chicken Breast 500g $5.99\", 3, 1),  # Meat (3), qty 1\n",
    "    (\"4x Bananas $2.40\", 0, 4)      # Fruit (0), qty 4\n",
    "]\n",
    "\n",
    "# Extracting sentences and labels for training\n",
    "sentences = [item[0] for item in hypothetical_data]\n",
    "category_labels_tensor = torch.tensor([item[1] for item in hypothetical_data], dtype=torch.long)\n",
    "quantity_labels_tensor = torch.tensor([item[2] for item in hypothetical_data], dtype=torch.float)\n",
    "\n",
    "# Training setup\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Using Adam optimizer for all trainable parameters\n",
    "classification_loss_fn = nn.CrossEntropyLoss()        # Cross Entropy loss function for classification task\n",
    "regression_loss_fn = nn.MSELoss()                     # MSE loss function for regression task\n",
    "num_epochs = 5                                        # Number of epochs for hypothetical training \n",
    "\n",
    "# Training loop (hypothetical run)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode (enables gradient tracking)\n",
    "    optimizer.zero_grad()  # Reset gradients to zero\n",
    "\n",
    "    # Forward pass: Get predictions for all sentences\n",
    "    class_logits, quantity_pred = model(sentences)\n",
    "    \n",
    "    # Computing losses for both tasks\n",
    "    class_loss = classification_loss_fn(class_logits, category_labels_tensor)\n",
    "    qty_loss = regression_loss_fn(quantity_pred.squeeze(), quantity_labels_tensor)\n",
    "    total_loss = class_loss + qty_loss  # Combine losses with equal weighting\n",
    "    \n",
    "    # Backward pass: Computing gradients\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # Updating model weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate metrics (accuracy for classification, MSE for quantity)\n",
    "    _, predicted_categories = torch.max(class_logits, 1)\n",
    "    accuracy = (predicted_categories == category_labels_tensor).float().mean().item()\n",
    "    mse = qty_loss.item()\n",
    "    \n",
    "    # Printing epoch progress (simulated output)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Classification Loss: {class_loss.item():.4f}, Quantity Loss: {qty_loss.item():.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, MSE: {mse:.4f}\")\n",
    "\n",
    "# Indication of hypothetical training completion\n",
    "print(\"Training complete (hypothetical run).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
