{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72245e6e-3bdb-4b26-92e4-a07c2cc84d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Required libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Imports for all tasks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"Setup complete. Required libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5264d9f3-9176-4b40-8f85-d043610c926b",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502f83f-98e6-4f4f-b7e5-c0c7d88a244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter sentences to generate embeddings (run this cell multiple times, then edit to add 'done'):\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Collect sentences\n",
    "sentences_task1 = []\n",
    "print(\"Enter sentences to generate embeddings (run this cell multiple times, then edit to add 'done'):\")\n",
    "sentence = input(\"> \")\n",
    "if sentence.lower() != 'done':\n",
    "    sentences_task1.append(sentence)\n",
    "else:\n",
    "    print(\"Finished collecting sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050c9fca-e725-4ceb-b64f-c8af6a161b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display embeddings\n",
    "model_task1 = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "if not sentences_task1:\n",
    "    print(\"No sentences provided.\")\n",
    "else:\n",
    "    embeddings = model_task1.encode(sentences_task1, batch_size=32, show_progress_bar=False)\n",
    "    print(\"\\nGenerated Sentence Embeddings:\")\n",
    "    print(\"-----------------------------\")\n",
    "    for i, (sentence, embedding) in enumerate(zip(sentences_task1, embeddings), 1):\n",
    "        print(f\"Sentence {i}: {sentence}\")\n",
    "        print(f\"Embedding (first 5 of 384 dimensions): {embedding[:5]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f770a2d-b672-48ea-80b9-b80f198ff8f0",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7b279-6298-49d0-a884-622dfe430c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Define the MTL model\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_categories=5):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.backbone = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.embedding_dim = 384\n",
    "        self.classification_head = nn.Linear(self.embedding_dim, num_categories)\n",
    "        self.regression_head = nn.Linear(self.embedding_dim, 1)\n",
    "    \n",
    "    def forward(self, sentences):\n",
    "        embeddings = self.backbone.encode(sentences, convert_to_tensor=True, \n",
    "                                        batch_size=32, show_progress_bar=False)\n",
    "        embeddings = embeddings.to(self.classification_head.weight.device)\n",
    "        class_logits = self.classification_head(embeddings)\n",
    "        quantity_pred = self.regression_head(embeddings)\n",
    "        return class_logits, quantity_pred\n",
    "\n",
    "# Initialize model\n",
    "model_task2 = MultiTaskModel(num_categories=5)\n",
    "category_labels = [\"Fruit\", \"Dairy\", \"Bakery\", \"Meat\", \"Other\"]\n",
    "print(\"MTL model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87e960-c083-4d94-9486-909566a4986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Hardcoded sentences and predictions\n",
    "sentences_task2 = [\n",
    "    \"2x Apples $1.99\", \"Milk 1L $2.50\", \"3x Bread $3.00\",\n",
    "    \"Chicken Breast 500g $5.99\", \"4x Bananas $2.40\"\n",
    "]\n",
    "\n",
    "print(\"Processing the following receipt items:\")\n",
    "for i, sentence in enumerate(sentences_task2, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "\n",
    "# Get predictions\n",
    "model_task2.eval()\n",
    "with torch.no_grad():\n",
    "    class_logits, quantity_pred = model_task2(sentences_task2)\n",
    "    class_probs = torch.softmax(class_logits, dim=1)\n",
    "    class_indices = torch.argmax(class_probs, dim=1)\n",
    "    \n",
    "    print(\"\\nMulti-Task Predictions:\")\n",
    "    print(\"----------------------\")\n",
    "    for i, (sentence, category_idx, qty) in enumerate(zip(sentences_task2, class_indices, quantity_pred)):\n",
    "        category = category_labels[category_idx.item()]\n",
    "        quantity = qty.item()\n",
    "        print(f\"Sentence {i+1}: {sentence}\")\n",
    "        print(f\"Predicted Category: {category}\")\n",
    "        print(f\"Predicted Quantity: {quantity:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f39178-49ab-40db-a166-b47ccc2ebed4",
   "metadata": {},
   "source": [
    "Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72aa2c-5873-4d2a-9792-7d6829ccbc93",
   "metadata": {},
   "source": [
    "Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a2daf-619d-4a8b-ab08-987ac07d1cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Training loop\n",
    "sentences_task4 = [\"I love ML!\", \"This is boring.\", \"Transformers rock!\"]\n",
    "labels_a = torch.tensor([1, 0, 1])  # Tech: 1, Non-Tech: 0\n",
    "labels_b = torch.tensor([1, 0, 1])  # Positive: 1, Negative: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52eb81d-b5d4-464e-b0db-fc171d8e1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize model for training (num_categories=2 for binary classification)\n",
    "model_task4 = MultiTaskModel(num_categories=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_task4.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_task4.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits_a, logits_b = model_task4(sentences_task4)\n",
    "    loss_a = loss_fn(logits_a, labels_a)\n",
    "    loss_b = loss_fn(logits_b, labels_b)\n",
    "    total_loss = loss_a + loss_b\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    preds_a = torch.argmax(logits_a, dim=1)\n",
    "    preds_b = torch.argmax(logits_b, dim=1)\n",
    "    acc_a = (preds_a == labels_a).float().mean().item()\n",
    "    acc_b = (preds_b == labels_b).float().mean().item()\n",
    "    print(f\"Epoch {epoch+1}/3\")\n",
    "    print(f\"Loss A: {loss_a.item():.4f}, Loss B: {loss_b.item():.4f}, Total Loss: {total_loss.item():.4f}\")\n",
    "    print(f\"Accuracy A: {acc_a:.4f}, Accuracy B: {acc_b:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
